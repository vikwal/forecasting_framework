data:
  path: 'data'
  freq: '1h'
  train_frac: 0.75
  val_frac: 0.25 # abs from train_frac
  test_start: '2019-03-01'
  scale_y: False
  lag_in_col: False

model:
  output_dim: 1
  lookback: 48
  horizon: 48
  batch_size: 16
  epochs: 50
  lr: 0.001
  optimizer: 'adam'
  loss: 'mse'
  metrics: ['mae', 'rmse']
  fnn:
    units: 128
    n_layers: 2
  cnn:
    filters: 64
    kernel_size: 3
    n_cnn_layers: 2
    increase_filters: False
  rnn:
    n_rnn_layers: 2
    units: 64
    dropout: 0.1
  tft:
    n_heads: 1
    hidden_dim: 45
    dropout: 0.1
  callbacks: False
  save_model: True
  shuffle: False
  force_retrain: True
  verbose: 1

hpo:
  studies_path: '/mnt/nasl1/optuna_studies.db'
  metric: 'rmse'
  trials: 1000
  kfolds: 5
  val_split: 0.75
  batch_size: [16, 256]
  epochs: [10, 50]
  n_layers: [1, 4]
  n_cnn_layers: [1, 8]
  n_rnn_layers: [1, 8]
  learning_rate: [0.0001, 0.1]
  fnn:
    units: [32, 256]
  cnn:
    filters: [16, 128]
    kernel_size: [2, 7]
    increase_filters: True
  rnn:
    units: [16, 300]
  tft:
    lookback: [24, 48, 72, 96]
    n_heads: [1, 6]
    hidden_dim: [8, 128]
    dropout: [0.0, 0.5]
  fl:
    strategy: 'fedadam'
    personalize: True
    n_rounds: [1,100]
    server_lr: [0.0001, 0.1]
    beta_1: [0,1]
    beta_2: [0,1]
    tau: [0.0001, 0.1]

eval:
  results_path: 'results'
  eval_on_all_test_data: True
  retrain_interval: 0 # in hours - 0 if no retraining is wanted
  t_0: 12

fl:
  parts: []
  n_clients: 7
  strategy: 'fedadam'
  n_rounds: 1
  personalize: False
  fedopt:
    server_lr: 0.001
    beta_1: 0.9
    beta_2: 0.99
    tau: 0.001
  force_retrain: True
  verbose: True
  save_history: False
  num_gpus: 1
  num_cpus: 11