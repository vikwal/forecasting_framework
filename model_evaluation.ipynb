{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Notebook\n",
    "\n",
    "Interactive notebook to evaluate trained models:\n",
    "- Select model from dropdown\n",
    "- Automatically load corresponding config\n",
    "- Build complete dataset (start ‚Üí test_end)\n",
    "- Load model and generate predictions\n",
    "\n",
    "**Model Naming Convention:**\n",
    "- `cl_m-tft_out-48_freq-1h_wind_50_static.pt` ‚Üí `config_wind_50.yaml` (with static features)\n",
    "- `cl_m-tft_out-48_freq-1h_wind_50_nostatic.pt` ‚Üí `config_wind_50.yaml` (no static features)\n",
    "- `cl_m-tft_out-48_freq-1h_wind_00164.pt` ‚Üí `config_wind_00164.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Interactive widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Local utilities\n",
    "from utils import preprocessing, tools, models, hpo\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set PyTorch settings\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter source: Config Defaults\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# HYPERPARAMETER SOURCE SELECTION\n",
    "# =====================================\n",
    "# Set this to control where hyperparameters are loaded from:\n",
    "# - False: Use standard hyperparameters from config['model'] (default for non-HPO models)\n",
    "# - True:  Load optimized hyperparameters from Optuna study (for HPO-trained models)\n",
    "\n",
    "use_hpo_hyperparameters = False  # ‚Üê Change this to True if model was trained with HPO\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "target_key = 'synth_00164.csv'\n",
    "\n",
    "print(f\"Hyperparameter source: {'Optuna Study (HPO)' if use_hpo_hyperparameters else 'Config Defaults'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def extract_config_name_from_model(model_filename):\n",
    "    \"\"\"\n",
    "    Extract config name from model filename.\n",
    "\n",
    "    Examples:\n",
    "        'cl_m-tft_out-48_freq-1h_wind_50_static.pt' -> 'config_wind_50'\n",
    "        'cl_m-tft_out-48_freq-1h_wind_00164.pt' -> 'config_wind_00164'\n",
    "\n",
    "    Returns:\n",
    "        config_name (str): Config filename without .yaml extension\n",
    "        use_static (bool): True if '_static' suffix, False if '_nostatic', None otherwise\n",
    "    \"\"\"\n",
    "    # Remove .pt extension\n",
    "    name = model_filename.replace('.pt', '')\n",
    "\n",
    "    # Check for static/nostatic suffix\n",
    "    use_static = None\n",
    "    if name.endswith('_static'):\n",
    "        use_static = True\n",
    "        name = name.replace('_static', '')\n",
    "    elif name.endswith('_nostatic'):\n",
    "        use_static = False\n",
    "        name = name.replace('_nostatic', '')\n",
    "\n",
    "    # Extract everything after '1h_'\n",
    "    match = re.search(r'1h_(.*)', name)\n",
    "    if match:\n",
    "        config_suffix = match.group(1)\n",
    "        config_name = f'config_{config_suffix}'\n",
    "        return config_name, use_static\n",
    "    else:\n",
    "        raise ValueError(f\"Could not extract config name from model filename: {model_filename}\")\n",
    "\n",
    "\n",
    "def load_config_for_model(model_filename, configs_dir='configs'):\n",
    "    \"\"\"\n",
    "    Load config file for given model.\n",
    "\n",
    "    Args:\n",
    "        model_filename: Model filename (e.g., 'cl_m-tft_out-48_freq-1h_wind_50_static.pt')\n",
    "        configs_dir: Directory containing config files\n",
    "\n",
    "    Returns:\n",
    "        config (dict): Loaded config\n",
    "        use_static (bool): Whether to use static features\n",
    "    \"\"\"\n",
    "    config_name, use_static = extract_config_name_from_model(model_filename)\n",
    "    config_path = os.path.join(configs_dir, f'{config_name}.yaml')\n",
    "\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    logger.info(f\"Loaded config: {config_path}\")\n",
    "    logger.info(f\"Static features mode: {use_static}\")\n",
    "\n",
    "    return config, use_static\n",
    "\n",
    "\n",
    "def adjust_static_features(config, use_static):\n",
    "    \"\"\"\n",
    "    Adjust static features based on model suffix.\n",
    "\n",
    "    Args:\n",
    "        config: Config dictionary\n",
    "        use_static: True to ensure static features, False to remove them, None to keep as-is\n",
    "\n",
    "    Returns:\n",
    "        Modified config\n",
    "    \"\"\"\n",
    "    if use_static is None:\n",
    "        # No suffix, use config as-is\n",
    "        return config\n",
    "\n",
    "    if use_static:\n",
    "        # Ensure static features are present (already in config, just verify)\n",
    "        logger.info(f\"Using static features: {config.get('params', {}).get('static_features', [])}\")\n",
    "    else:\n",
    "        # Remove static features\n",
    "        if 'params' in config and 'static_features' in config['params']:\n",
    "            config['params']['static_features'] = []\n",
    "            logger.info(\"Removed static features (nostatic mode)\")\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 model(s):\n",
      "  - cl_m-tft_out-48_freq-1h_wind_00164.pt\n",
      "  - cl_m-tft_out-48_freq-1h_wind_02638.pt\n",
      "  - cl_m-tft_out-48_freq-1h_wind_eight_nostatic.pt\n",
      "  - cl_m-tft_out-48_freq-1h_wind_eight_static.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e2d21ea12646c395b6e241a9c50b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select Model:', layout=Layout(width='600px'), options=('cl_m-tft_out-48_freq-1h_wind_001‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get all .pt model files\n",
    "models_dir = 'models'\n",
    "model_files = sorted([f for f in os.listdir(models_dir) if f.endswith('.pt')])\n",
    "\n",
    "if not model_files:\n",
    "    print(f\"‚ö†Ô∏è No .pt model files found in {models_dir}/\")\n",
    "else:\n",
    "    print(f\"Found {len(model_files)} model(s):\")\n",
    "    for mf in model_files:\n",
    "        print(f\"  - {mf}\")\n",
    "\n",
    "# Create dropdown widget\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=model_files,\n",
    "    description='Select Model:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "# Display widget\n",
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Config and Build Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 16:54:22,726 - INFO - Loaded config: configs/config_wind_eight.yaml\n",
      "2025-12-04 16:54:22,727 - INFO - Static features mode: True\n",
      "2025-12-04 16:54:22,728 - INFO - Using static features: ['park_age', 'cut_in', 'cut_out', 'rated_wind_speed', 'hub_height', 'rotor_diameter', 'altitude']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: cl_m-tft_out-48_freq-1h_wind_eight_static.pt\n",
      "\n",
      "üèóÔ∏è Model architecture: TFT\n",
      "\n",
      "üìã Features configuration:\n",
      "  Known features (3): ['wind_speed_h78', 'wind_speed_h127', 'wind_speed_h184']\n",
      "  Observed features (1): ['power']\n",
      "  Static features (7): ['park_age', 'cut_in', 'cut_out', 'rated_wind_speed', 'hub_height', 'rotor_diameter', 'altitude']\n"
     ]
    }
   ],
   "source": [
    "# Get selected model\n",
    "selected_model = model_dropdown.value\n",
    "print(f\"Selected model: {selected_model}\")\n",
    "\n",
    "# Load corresponding config\n",
    "config, use_static = load_config_for_model(selected_model)\n",
    "\n",
    "# Adjust static features based on suffix\n",
    "config = adjust_static_features(config, use_static)\n",
    "\n",
    "# Handle frequency\n",
    "config = tools.handle_freq(config=config)\n",
    "\n",
    "# Set model name for preprocessing\n",
    "model_name_match = re.search(r'm-([a-z]+)', selected_model)\n",
    "if model_name_match:\n",
    "    model_architecture = model_name_match.group(1)\n",
    "    config['model']['name'] = model_architecture\n",
    "    print(f\"\\nüèóÔ∏è Model architecture: {model_architecture.upper()}\")\n",
    "else:\n",
    "    raise ValueError(f\"Could not extract model architecture from filename: {selected_model}\")\n",
    "\n",
    "# Get features\n",
    "features = preprocessing.get_features(config=config)\n",
    "\n",
    "print(\"\\nüìã Features configuration:\")\n",
    "print(f\"  Known features ({len(features['known'])}): {features['known']}\")\n",
    "print(f\"  Observed features ({len(features['observed'])}): {features['observed']}\")\n",
    "print(f\"  Static features ({len(features['static'])}): {features['static']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Complete Dataset\n",
    "\n",
    "Load raw data dictionary and process analog to `train_cl.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Building complete dataset...\n",
      "\n",
      "Data config:\n",
      "  Data directory: /mnt/nas/synthetic/wind/wind_hourly_age_20251103\n",
      "  Frequency: 1h\n",
      "  Test end: 2025-10-21 00:00:00+00:00\n",
      "  Target column: power\n",
      "\n",
      "üîÑ Loading raw data (dictionary of DataFrames)...\n",
      "‚úì Loaded 8 DataFrame(s):\n",
      "  synth_00164.csv: (156912, 11)\n",
      "    Date range: 2023-07-24 06:00:00+00:00 to 2025-10-22 15:00:00+00:00\n",
      "    Columns: ['wind_speed_h78_1', 'wind_speed_h127_1', 'wind_speed_h184_1', 'power', 'park_age', 'cut_in', 'cut_out', 'rated_wind_speed', 'hub_height', 'rotor_diameter', 'altitude']\n",
      "  synth_03362.csv: (156912, 11)\n",
      "    Date range: 2023-07-24 06:00:00+00:00 to 2025-10-22 15:00:00+00:00\n",
      "    Columns: ['wind_speed_h78_1', 'wind_speed_h127_1', 'wind_speed_h184_1', 'power', 'park_age', 'cut_in', 'cut_out', 'rated_wind_speed', 'hub_height', 'rotor_diameter', 'altitude']\n",
      "  synth_03631.csv: (156912, 11)\n",
      "    Date range: 2023-07-24 06:00:00+00:00 to 2025-10-22 15:00:00+00:00\n",
      "    Columns: ['wind_speed_h78_1', 'wind_speed_h127_1', 'wind_speed_h184_1', 'power', 'park_age', 'cut_in', 'cut_out', 'rated_wind_speed', 'hub_height', 'rotor_diameter', 'altitude']\n",
      "  synth_07370.csv: (156912, 11)\n",
      "    Date range: 2023-07-24 06:00:00+00:00 to 2025-10-22 15:00:00+00:00\n",
      "    Columns: ['wind_speed_h78_1', 'wind_speed_h127_1', 'wind_speed_h184_1', 'power', 'park_age', 'cut_in', 'cut_out', 'rated_wind_speed', 'hub_height', 'rotor_diameter', 'altitude']\n",
      "  synth_02638.csv: (156912, 11)\n",
      "    Date range: 2023-07-24 06:00:00+00:00 to 2025-10-22 15:00:00+00:00\n",
      "    Columns: ['wind_speed_h78_1', 'wind_speed_h127_1', 'wind_speed_h184_1', 'power', 'park_age', 'cut_in', 'cut_out', 'rated_wind_speed', 'hub_height', 'rotor_diameter', 'altitude']\n",
      "  synth_02932.csv: (156912, 11)\n",
      "    Date range: 2023-07-24 06:00:00+00:00 to 2025-10-22 15:00:00+00:00\n",
      "    Columns: ['wind_speed_h78_1', 'wind_speed_h127_1', 'wind_speed_h184_1', 'power', 'park_age', 'cut_in', 'cut_out', 'rated_wind_speed', 'hub_height', 'rotor_diameter', 'altitude']\n",
      "  synth_06163.csv: (156912, 11)\n",
      "    Date range: 2023-07-24 06:00:00+00:00 to 2025-10-22 15:00:00+00:00\n",
      "    Columns: ['wind_speed_h78_1', 'wind_speed_h127_1', 'wind_speed_h184_1', 'power', 'park_age', 'cut_in', 'cut_out', 'rated_wind_speed', 'hub_height', 'rotor_diameter', 'altitude']\n",
      "  synth_07374.csv: (156912, 11)\n",
      "    Date range: 2023-07-24 06:00:00+00:00 to 2025-10-22 15:00:00+00:00\n",
      "    Columns: ['wind_speed_h78_1', 'wind_speed_h127_1', 'wind_speed_h184_1', 'power', 'park_age', 'cut_in', 'cut_out', 'rated_wind_speed', 'hub_height', 'rotor_diameter', 'altitude']\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Building complete dataset...\\n\")\n",
    "\n",
    "# Get data parameters\n",
    "data_config = config.get('data', {})\n",
    "test_end = pd.Timestamp(data_config.get('test_end'), tz='UTC')\n",
    "freq = data_config.get('freq', '1h')\n",
    "target_col = data_config.get('target_col', 'power')\n",
    "data_dir = data_config.get('path')\n",
    "\n",
    "print(f\"Data config:\")\n",
    "print(f\"  Data directory: {data_dir}\")\n",
    "print(f\"  Frequency: {freq}\")\n",
    "print(f\"  Test end: {test_end}\")\n",
    "print(f\"  Target column: {target_col}\")\n",
    "\n",
    "# Load raw data - returns dictionary of DataFrames\n",
    "print(\"\\nüîÑ Loading raw data (dictionary of DataFrames)...\")\n",
    "dfs = preprocessing.get_data(\n",
    "    data_dir=data_dir,\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    features=features\n",
    ")\n",
    "\n",
    "print(f\"‚úì Loaded {len(dfs)} DataFrame(s):\")\n",
    "for key, df in dfs.items():\n",
    "    print(f\"  {key}: {df.shape}\")\n",
    "    print(f\"    Date range: {df.index.get_level_values(0).min()} to {df.index.get_level_values(0).max()}\")\n",
    "    print(f\"    Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Train and Test Data\n",
    "\n",
    "Process each DataFrame through the pipeline and combine, analog to `train_cl.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Processing data through pipeline...\n",
      "\n",
      "Dataset split (from config):\n",
      "  Test start: 2025-08-01 00:00:00+00:00\n",
      "  Test end: 2025-10-21 00:00:00+00:00\n",
      "  Train fraction: 1\n",
      "\n",
      "Model parameters:\n",
      "  Output dim: 48\n",
      "  Lookback: 48\n",
      "  Horizon: 48\n",
      "  Step size: 48\n",
      "\n",
      "üìê Fitting global scaler on training data only...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Global Scaler: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:05<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Global scaler fitted on training data\n",
      "\n",
      "üîÑ Creating data generator and combining datasets...\n",
      "\n",
      "üìä Data shapes:\n",
      "  X_train known: (23336, 96, 3), X_test known: (2592, 96, 3)\n",
      "  X_train static: (23336, 7), X_test static: (2592, 7)\n",
      "  X_train observed: (23336, 48, 1), X_test observed: (2592, 48, 1)\n",
      "  y_train: (23336, 48), y_test: (2592, 48)\n",
      "\n",
      "‚úì Per-park test data available:\n",
      "  synth_00164.csv\n",
      "  synth_03362.csv\n",
      "  synth_03631.csv\n",
      "  synth_07370.csv\n",
      "  synth_02638.csv\n",
      "  synth_02932.csv\n",
      "  synth_06163.csv\n",
      "  synth_07374.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"üîß Processing data through pipeline...\\n\")\n",
    "\n",
    "# Use config values as-is (same as during training!)\n",
    "test_start = pd.Timestamp(config['data']['test_start'], tz='UTC')\n",
    "test_end = pd.Timestamp(config['data']['test_end'], tz='UTC')\n",
    "train_frac = config['data']['train_frac']\n",
    "\n",
    "print(f\"Dataset split (from config):\")\n",
    "print(f\"  Test start: {test_start}\")\n",
    "print(f\"  Test end: {test_end}\")\n",
    "print(f\"  Train fraction: {train_frac}\")\n",
    "\n",
    "# Get model parameters\n",
    "model_config = config.get('model', {})\n",
    "output_dim = model_config.get('output_dim', 48)\n",
    "lookback = model_config.get('lookback', 48)\n",
    "horizon = model_config.get('horizon', 48)\n",
    "step_size = model_config.get('step_size', 1)\n",
    "\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Output dim: {output_dim}\")\n",
    "print(f\"  Lookback: {lookback}\")\n",
    "print(f\"  Horizon: {horizon}\")\n",
    "print(f\"  Step size: {step_size}\")\n",
    "\n",
    "# Fit global scaler on TRAINING data only (analog to train_cl.py)\n",
    "print(\"\\nüìê Fitting global scaler on training data only...\")\n",
    "global_scaler_x = StandardScaler()\n",
    "\n",
    "for key, df in tqdm(dfs.items(), desc=\"Fitting Global Scaler\"):\n",
    "    df_temp = df.copy()\n",
    "\n",
    "    # For non-TFT models, create lag features before fitting scaler\n",
    "    if config['model']['name'] not in ['tft', 'stemgnn']:\n",
    "        for col in features['observed']:\n",
    "            all_observed_cols = [new_col for new_col in df_temp.columns if col in new_col]\n",
    "            for new_col in all_observed_cols:\n",
    "                df_temp = preprocessing.lag_features(\n",
    "                    data=df_temp,\n",
    "                    lookback=lookback,\n",
    "                    horizon=horizon,\n",
    "                    lag_in_col=config['data']['lag_in_col'],\n",
    "                    target_col=new_col\n",
    "                )\n",
    "                # Drop the original column if it's not the target and not known\n",
    "                if new_col != target_col and new_col not in features['known']:\n",
    "                    df_temp.drop(new_col, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # Drop target column\n",
    "    if target_col in df_temp.columns:\n",
    "        df_temp.drop(target_col, axis=1, inplace=True)\n",
    "\n",
    "    # Split data to get ONLY training portion for scaler fitting\n",
    "    t_0 = 0 if config['eval']['eval_on_all_test_data'] else config['eval']['t_0']\n",
    "    df_train, _ = preprocessing.split_data(\n",
    "        data=df_temp,\n",
    "        train_frac=train_frac,\n",
    "        test_start=test_start,\n",
    "        test_end=test_end,\n",
    "        t_0=t_0\n",
    "    )\n",
    "    global_scaler_x.partial_fit(df_train)\n",
    "\n",
    "    del df_temp, df_train\n",
    "    gc.collect()\n",
    "\n",
    "print(\"‚úì Global scaler fitted on training data\")\n",
    "\n",
    "# Process data analog to train_cl.py\n",
    "print(\"\\nüîÑ Creating data generator and combining datasets...\")\n",
    "data_generator = tools.create_data_generator(dfs, config, features, scaler_x=global_scaler_x)\n",
    "X_train, y_train, X_test, y_test, test_data = tools.combine_datasets_efficiently(data_generator)\n",
    "\n",
    "# Log shapes\n",
    "if isinstance(X_train, dict):\n",
    "    if 'known' in X_train:\n",
    "        print(f'\\nüìä Data shapes:')\n",
    "        print(f'  X_train known: {X_train[\"known\"].shape}, X_test known: {X_test[\"known\"].shape}')\n",
    "    if 'static' in X_train:\n",
    "        print(f'  X_train static: {X_train[\"static\"].shape}, X_test static: {X_test[\"static\"].shape}')\n",
    "    print(f'  X_train observed: {X_train[\"observed\"].shape}, X_test observed: {X_test[\"observed\"].shape}')\n",
    "    print(f'  y_train: {y_train.shape}, y_test: {y_test.shape}')\n",
    "else:\n",
    "    print(f'\\nüìä Data shapes:')\n",
    "    print(f'  X_train: {X_train.shape}, X_test: {X_test.shape}')\n",
    "    print(f'  y_train: {y_train.shape}, y_test: {y_test.shape}')\n",
    "\n",
    "print(f\"\\n‚úì Per-park test data available:\")\n",
    "for park_key in test_data.keys():\n",
    "    print(f\"  {park_key}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Model and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading trained model...\n",
      "\n",
      "Feature dimensions: {'observed_dim': 1, 'known_dim': 3, 'static_dim': 7}\n",
      "\n",
      "Using device: cuda\n",
      "\n",
      "üìã Using standard hyperparameters from config\n",
      "  hidden_dim: 16\n",
      "  n_heads: 4\n",
      "üìù Cleaning state_dict (removing _orig_mod. prefix from compiled model)\n",
      "\n",
      "üèóÔ∏è Creating TFT model...\n",
      "‚úì Model loaded successfully!\n",
      "  Parameters: 36,407\n"
     ]
    }
   ],
   "source": [
    "print(\"ü§ñ Loading trained model...\\n\")\n",
    "\n",
    "# Set feature_dim in config (required for model instantiation)\n",
    "config['model']['feature_dim'] = tools.get_feature_dim(X=X_train)\n",
    "print(f\"Feature dimensions: {config['model']['feature_dim']}\")\n",
    "\n",
    "# Get device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Load hyperparameters based on user selection\n",
    "if use_hpo_hyperparameters:\n",
    "    # Try to load from Optuna study\n",
    "    study_name_suffix = '_'.join(extract_config_name_from_model(selected_model)[0].split('_')[1:])\n",
    "    study_name = f'cl_m-{model_architecture}_out-{output_dim}_freq-{freq}_{study_name_suffix}'\n",
    "    print(f\"\\nüìö Attempting to load hyperparameters from study: {study_name}\")\n",
    "\n",
    "    study = hpo.load_study(config['hpo']['studies_path'], study_name)\n",
    "    hyperparameters = hpo.get_hyperparameters(config=config, study=study)\n",
    "    print(\"‚úì Loaded hyperparameters from Optuna study\")\n",
    "    print(f\"  hidden_dim: {hyperparameters.get('hidden_dim', 'N/A')}\")\n",
    "    print(f\"  n_heads: {hyperparameters.get('n_heads', 'N/A')}\")\n",
    "else:\n",
    "    # Use standard hyperparameters from config\n",
    "    hyperparameters = hpo.get_hyperparameters(config=config, study=None)\n",
    "    print(f\"\\nüìã Using standard hyperparameters from config\")\n",
    "    print(f\"  hidden_dim: {hyperparameters.get('hidden_dim', 'N/A')}\")\n",
    "    print(f\"  n_heads: {hyperparameters.get('n_heads', 'N/A')}\")\n",
    "\n",
    "# Load model checkpoint\n",
    "model_path = os.path.join(models_dir, selected_model)\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Checkpoint is just state_dict (no metadata)\n",
    "model_state = checkpoint\n",
    "\n",
    "# Remove '_orig_mod.' prefix if model was trained with torch.compile()\n",
    "if any(k.startswith('_orig_mod.') for k in model_state.keys()):\n",
    "    print(\"üìù Cleaning state_dict (removing _orig_mod. prefix from compiled model)\")\n",
    "    model_state = {k.replace('_orig_mod.', ''): v for k, v in model_state.items()}\n",
    "\n",
    "# Create model instance with selected hyperparameters\n",
    "print(f\"\\nüèóÔ∏è Creating {model_architecture.upper()} model...\")\n",
    "model = models.get_model(config, hyperparameters)\n",
    "model.load_state_dict(model_state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úì Model loaded successfully!\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Predictions for All Parks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÆ Generating predictions for all parks...\n",
      "\n",
      "Processing park: synth_00164.csv\n",
      "  ‚úì Predictions shape: (324, 48)\n",
      "  ‚úì Index range: 2025-08-01 06:00:00+00:00 to 2025-10-20 15:00:00+00:00\n",
      "\n",
      "‚úÖ Predictions generated for 1 park(s)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîÆ Generating predictions for all parks...\\n\")\n",
    "\n",
    "# Store predictions per park\n",
    "predictions_per_park = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for park_key, (X_test_park, y_test_park, index_test_park, scaler_y_park) in test_data.items():\n",
    "\n",
    "        if park_key != target_key:\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing park: {park_key}\")\n",
    "\n",
    "        # Get predictions\n",
    "        y_true, y_pred = tools.get_y(\n",
    "            X_test=X_test_park,\n",
    "            y_test=y_test_park,\n",
    "            model=model,\n",
    "            scaler_y=scaler_y_park,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Store predictions\n",
    "        predictions_per_park[park_key] = {\n",
    "            'X': X_test_park,\n",
    "            'y_true': y_test_park,\n",
    "            'y_pred': y_pred,\n",
    "            'index': index_test_park,\n",
    "            'scaler_y': scaler_y_park,\n",
    "            'raw_df': dfs[park_key]\n",
    "        }\n",
    "\n",
    "        print(f\"  ‚úì Predictions shape: {y_pred.shape}\")\n",
    "        print(f\"  ‚úì Index range: {index_test_park.min()} to {index_test_park.max()}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Predictions generated for {len(predictions_per_park)} park(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation\n",
    "\n",
    "Evaluation of the models is done in the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Calculating metrics by forecast hour...\n",
      "\n",
      "             R¬≤      RMSE       MAE\n",
      "06:00  0.604689  0.056760  0.031821\n",
      "09:00  0.616284  0.055950  0.031824\n",
      "12:00  0.586380  0.058100  0.032348\n",
      "15:00  0.586889  0.058067  0.032213\n",
      "Mean   0.598561  0.057219  0.032052\n",
      "\n",
      "‚úì Metrics calculated\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Calculating metrics by forecast hour...\\n\")\n",
    "\n",
    "# Get test indices to group by hour\n",
    "test_indices = test_data[target_key][2]\n",
    "\n",
    "# Group predictions by forecast hour (from starttime)\n",
    "metrics_by_hour = {}\n",
    "\n",
    "for hour in ['06:00', '09:00', '12:00', '15:00']:\n",
    "    # Find indices for this hour\n",
    "    hour_mask = [idx.strftime('%H:%M') == hour for idx in test_indices]\n",
    "\n",
    "    if not any(hour_mask):\n",
    "        continue\n",
    "\n",
    "    # Get predictions and actuals for this hour\n",
    "    y_true_hour = y_true[hour_mask].flatten()\n",
    "    y_pred_hour = y_pred[hour_mask].flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_true_hour, y_pred_hour)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_hour, y_pred_hour))\n",
    "    mae = mean_absolute_error(y_true_hour, y_pred_hour)\n",
    "\n",
    "    metrics_by_hour[hour] = {'R¬≤': r2, 'RMSE': rmse, 'MAE': mae}\n",
    "\n",
    "# Calculate overall mean\n",
    "all_metrics = list(metrics_by_hour.values())\n",
    "mean_r2 = np.mean([m['R¬≤'] for m in all_metrics])\n",
    "mean_rmse = np.mean([m['RMSE'] for m in all_metrics])\n",
    "mean_mae = np.mean([m['MAE'] for m in all_metrics])\n",
    "\n",
    "metrics_by_hour['Mean'] = {'R¬≤': mean_r2, 'RMSE': mean_rmse, 'MAE': mean_mae}\n",
    "\n",
    "# Create DataFrame for nice display\n",
    "metrics_df = pd.DataFrame(metrics_by_hour).T\n",
    "print(metrics_df.to_string())\n",
    "print(\"\\n‚úì Metrics calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Creating forecast plots...\n",
      "\n",
      "Loading turbine wind speeds from synth data...\n",
      "Loaded 20016 turbine wind speed values\n",
      "NWP wind columns to plot: ['wind_speed_h127_1', 'wind_speed_h184_1', 'wind_speed_h78_1']\n",
      "Heights: ['127', '184', '78']\n",
      "Generating 324 plots...\n",
      "  Generated 50/324 plots...\n",
      "  Generated 100/324 plots...\n",
      "  Generated 150/324 plots...\n",
      "  Generated 200/324 plots...\n",
      "  Generated 250/324 plots...\n",
      "  Generated 300/324 plots...\n",
      "\n",
      "‚úì All plots saved to: figs/cl_m-tft_out-48_freq-1h_wind_eight_static/\n"
     ]
    }
   ],
   "source": [
    "print(\"üìà Creating forecast plots...\\n\")\n",
    "\n",
    "# Create output directory\n",
    "model_name = selected_model.replace('.pt', '')\n",
    "output_dir = f'figs/{model_name}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get the original dataframe for wind speeds\n",
    "park_df = dfs[park_key]\n",
    "\n",
    "# Load turbine wind speed from synth CSV\n",
    "print(\"Loading turbine wind speeds from synth data...\")\n",
    "park_id = park_key.replace('synth_', '').replace('.csv', '')\n",
    "synth_path = os.path.join(config['data']['path'], f'synth_{park_id}.csv')\n",
    "\n",
    "# Load synth data with timestamp as index\n",
    "synth_df = pd.read_csv(synth_path, sep=';')\n",
    "synth_df['timestamp'] = pd.to_datetime(synth_df['timestamp'])\n",
    "synth_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Keep only wind_speed column\n",
    "turbine_wind = synth_df[['wind_speed']].copy()\n",
    "print(f\"Loaded {len(turbine_wind)} turbine wind speed values\")\n",
    "\n",
    "# Find wind speed columns (NWP with 'h')\n",
    "wind_cols_nwp = [col for col in park_df.columns if 'wind_speed' in col and '_h' in col]\n",
    "wind_cols_nwp = sorted(wind_cols_nwp)[:3]  # Take first 3\n",
    "\n",
    "# Extract heights from column names for legend\n",
    "nwp_heights = []\n",
    "for col in wind_cols_nwp:\n",
    "    match = re.search(r'_h(\\d+)', col)\n",
    "    if match:\n",
    "        nwp_heights.append(match.group(1))\n",
    "\n",
    "print(f\"NWP wind columns to plot: {wind_cols_nwp}\")\n",
    "print(f\"Heights: {nwp_heights}\")\n",
    "\n",
    "# Iterate over all test samples\n",
    "n_samples = len(y_true)\n",
    "print(f\"Generating {n_samples} plots...\")\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Get starttime for this forecast\n",
    "    starttime = test_indices[i]\n",
    "\n",
    "    # Get predictions and actuals\n",
    "    y_true_sample = y_true[i]\n",
    "    y_pred_sample = y_pred[i]\n",
    "\n",
    "    # Get wind speeds from original dataframe (NWP forecasts)\n",
    "    starttime_mask = park_df.index.get_level_values('starttime') == starttime\n",
    "    wind_data_nwp = park_df.loc[starttime_mask, wind_cols_nwp]\n",
    "\n",
    "    # Get timestamps for this forecast to fetch turbine wind speeds\n",
    "    forecast_timestamps = park_df.loc[starttime_mask].index.get_level_values('timestamp')\n",
    "\n",
    "    # Fetch turbine wind speeds for these timestamps\n",
    "    wind_data_turbine = []\n",
    "    for ts in forecast_timestamps:\n",
    "        if ts in turbine_wind.index:\n",
    "            wind_data_turbine.append(turbine_wind.loc[ts, 'wind_speed'])\n",
    "        else:\n",
    "            wind_data_turbine.append(np.nan)\n",
    "\n",
    "    # Create figure with two y-axes\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7), dpi=300)\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # X-axis: t+1 to t+48\n",
    "    x = np.arange(1, 49)\n",
    "\n",
    "    # Left y-axis: Power (y_true and y_pred)\n",
    "    ax1.plot(x, y_true_sample, 'k-', linewidth=2.5, label='True', alpha=0.9)  # Black\n",
    "    ax1.plot(x, y_pred_sample, 'b-', linewidth=2, label='Predicted', alpha=0.8)\n",
    "    ax1.set_xlabel('Forecast Horizon (hours)', fontsize=13)\n",
    "    ax1.set_ylabel('Power (normalized)', fontsize=13, color='black')\n",
    "    ax1.tick_params(axis='y', labelcolor='black')\n",
    "    ax1.set_xticks(np.arange(0, 49, 6))  # 6-hour steps\n",
    "    ax1.legend(loc='upper left', fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Right y-axis: Wind speeds\n",
    "    colors_nwp = ['#ff7f0e', '#ff9f4e', '#ffbf7e']  # Orange gradient\n",
    "\n",
    "    # Plot NWP wind speeds (dashed)\n",
    "    for idx, (col, color, height) in enumerate(zip(wind_cols_nwp, colors_nwp, nwp_heights)):\n",
    "        if len(wind_data_nwp) == 48:\n",
    "            ax2.plot(x, wind_data_nwp[col].values, '--', color=color,\n",
    "                    linewidth=1.8, label=f'NWP {height}m', alpha=0.75)\n",
    "\n",
    "    # Plot turbine hub height wind speed (solid red)\n",
    "    if len(wind_data_turbine) == 48:\n",
    "        ax2.plot(x, wind_data_turbine, '-', color='#7f7f7f',\n",
    "                linewidth=2.2, label='Measured 10m', alpha=0.95)\n",
    "\n",
    "    ax2.set_ylabel('Wind Speed (m/s)', fontsize=13, color='#ff7f0e')\n",
    "    ax2.tick_params(axis='y', labelcolor='#ff7f0e')\n",
    "    ax2.legend(loc='upper right', fontsize=10)\n",
    "\n",
    "    # Title\n",
    "    plt.title(f'Forecast starting: {starttime.strftime(\"%Y-%m-%d %H:%M\")}',\n",
    "             fontsize=15, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    filename = starttime.strftime(\"%Y-%m-%d %H_%M_%S\") + '.png'\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Progress indicator\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Generated {i+1}/{n_samples} plots...\")\n",
    "\n",
    "print(f\"\\n‚úì All plots saved to: {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frcst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
